{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Interrogating BERT 1. Classification.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1554842072433}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dCpvgG0vwXAZ"},"source":["# Interrogating BERT (Classification)\n","\n","By Peter Usherwood, \n","\n","Adpated from the official tutorial found here https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xiYrZKaHwV81"},"source":["# 0.0 Intro\n","\n","BERT is currently one of (if not top of the) leaders in NLP transfer learning. To get a full picture on what BERT is I reccomend refering to the original paper https://arxiv.org/abs/1810.04805 or this excellent blog post on the NLP transfer learning zoo http://jalammar.github.io/illustrated-bert/. Here we going through a more technical user guide of how to do transfer learning with BERT. The target audience for this is data scientists who understand a bit of tensorflow/python/nlp/deep learning already.\n","\n","This \"tutorial\" is split into 2 notebooks, this is the first where we shall see how BERT works by looking at doing a simple classification transfer learning task. However I have dedicated a whole second notebook to understanding how to use BERT for sequence to sequence tasks (specifically question answering with a Facebook dataset) because in my opinion it is such a maaaaaaaassive pain it needs a whole extra notebook. However I wont re-cover things in here in that one so please read both :)\n","\n","The purpose of this notebook/tutorial is interrogate BERT to understand exactly what he is, how he works, and how we can use him. Here a lot of this notebook is based on the official tutorial for BERT, and we do use the official BERT library, however.... whenever the BERT library boilerplate is used to hide what is going on we pull it apart to understand exactly what is going on at each stage and how we 'could' modify it. The goal here is to give a comprehensive technical tutorial for transfer learning with BERT so that we can use it in the most project/data agnostic way possible, after we could go back to using boilerplate or not depending on our project. As such you will see the phrase 'BERT black box' used quite disparagingly here, however it is more used as a reference point for where were attempting to de-mistify the high level apis.\n","\n","Now when I say comprehensive technical tutorial, I am still treating this from the transfer learning side, I am not here diving down into how the core net works, just fully understanding the inputs, how we join the core net into a bigger model, how we train and evaluate it, and how we can use all of that in a pipeline. \n","\n","I built this tutorial based on the great official one found here, if you just want a quick working example I reccomend reading that instead (this one's more wordy) https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n","\n","Now not all of the high level apis here are BERTs fault, and they may not even be vexing to you. Specifically Estimators and Tensorflow Hub are used extensively here. These are not super new but new enough that they were confusing to me, as such I will lightly tackle these at the same time, particularaly in places where the BERT black box overlaps so we fully appriciate whats going on.\n","\n","- NOTE: Connecting to hub can be a problem on some Kantar networks, if the code in tokenizer or model hangs, take your laptop home and try there.\n","- NOTE: You need Tensorflow 1.11.0> for BERT\n","- NOTE: Unless you've got a nice graphics card, probably its best to upload this and the data to colab and run it there\n","- NOTE: it is important to remember BERT is not a person when you read this notebook"]},{"cell_type":"markdown","metadata":{"id":"a-SbBG6v1HbX","colab_type":"text"},"source":["# 0.1 How fat is BERT?\n","\n","This is a key question, it's all well and good understanding the thoery or the technical detail but if we cant run BERT on are hardware it's not much good. So how fat is he? Well unfortunately... quite. As said on the official github repo:\n","\n","\"Important: All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. We are working on adding code to this repository which allows for much larger effective batch size on the GPU. See the section on out-of-memory issues for more details.\n","\n","This code was tested with TensorFlow 1.11.0. It was tested with Python2 and Python3 (but more thoroughly with Python2, since this is what's used internally in Google).\n","\n","The fine-tuning examples which use BERT-Base should be able to run on a GPU that has at least 12GB of RAM using the hyperparameters given.\"\n","\n","To put this in perspective most mid-range laptops with GPUs will have about 4GB VRAM (as per mine). And the best \"reasonable\" consumer graphics card you'd buy is the RTX 2080Ti which has 11GB and costs like $1200 (if you live in the US, where I am its like 2000). Now google colab do allow you to have up to I think its about 12GB VRAM so you can run BERT-small there (which is what im doing in this tutorial), but its important to bare this in mind when making producing classifiers with BERT."]},{"cell_type":"markdown","metadata":{"id":"6MDYrU5k1HbY","colab_type":"text"},"source":["# 0.2 How fast is BERT?\n","\n","So to train the base language model in BERT is quite slow, as expected its a huge transformer neural net that runs forward and back passes and does next sentence prediction. As per the published paper:\n","\n","\"Training of BERTBASE was performed on 4\n","Cloud TPUs in Pod configuration (16 TPU chips\n","total).5 Training of BERTLARGE was performed\n","on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.\"\n","\n","However thats not really the focus of this tutorial (although something to bare in mind if you were thinking of training a BERT in a different language). Here we're focusing more on transfer learning using a pre-trained BERT and the good news here is it is quite fast. Training the classification example here with 1500 labeled examples only takes of the order 5-10 minutes, and this should scale linearly with number of examples (you could obviously up the number of epochs too). So in conclusion once you have the base, BERT is quite fast, if you have a graphics card that can hold him training time is not an issue."]},{"cell_type":"markdown","metadata":{"id":"ssVsHRL81HbZ","colab_type":"text"},"source":["# 0.3 How good is BERT at school?\n","\n","Anybody thats been following NLP knows BERT got SOTAs across the board on various language tasks, one thing of particular interest is that the architecture allows for seq -> seq tasks as well as classification which makes it more felixble than some of the others (ULMFiT for example). One thing that is not fully clear to me yet is how good BERT (or indeed other transfer learning techniques) are when put under \"extreme\" conditions, for example low shot learning, dirty or noisy text data like social, how good they are going cross domain etc. Im looking into this, but thats for another time, all that is important for now is, it seems BERT is one of the best at NLP ML problems today."]},{"cell_type":"markdown","metadata":{"id":"2J6BFuH31Hba","colab_type":"text"},"source":["# 0.4 How do we make him dance?\n","\n","This is the main purpose of this repo, its understanding how BERT works and how we can use it to build out useful classifiers and seq -> seq tasks. As per the official tutorial we start by looking at a classification problem (this notebook), although here we use trinary data and not binary and a different dataset (Yelp) to ensure things are agnostic as possible. After we will look at seq -> seq for contextual question answering (in the second notebook)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hsZvic2YxnTz","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub # pip install tensorflow-hub\n","from datetime import datetime"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hhbGEfwgdEtw","outputId":"e3fa4131-e9ee-4602-89ae-2dfb883b50be","executionInfo":{"status":"ok","timestamp":1564496537788,"user_tz":180,"elapsed":8107,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["!pip install bert-tensorflow\n","import bert # pip install bert-tensorflow\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\u001b[K     |████████████████████████████████| 71kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0730 14:22:17.520803 140708289415040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sX_jjO8Y1boX","colab_type":"code","outputId":"d8fcf6b7-d2b6-4b57-8ec1-45a6719a955a","executionInfo":{"status":"ok","timestamp":1564496817565,"user_tz":180,"elapsed":287858,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["# Only run this if your doing this on colab, you will have to upload the data to drive too\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pmFYvkylMwXn"},"source":["# 1.0 Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MC_w8SRqN0fr"},"source":["First we get some data, nothing here is BERT specific, just getting any old classification data, this comes from Yelp and has been balanced and only 1, 3, and 5 star reviews are included"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fom_ff20gyy6","outputId":"4728760f-f370-4047-954c-0e8507a83107","executionInfo":{"status":"error","timestamp":1561768885649,"user_tz":180,"elapsed":1943,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["df_trn = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/train.csv\")\n","df_val = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/validate.csv\")\n","df_tst = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/test.csv\")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1366db3500af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_trn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/validate.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/train.csv' does not exist: b'/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/train.csv'"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XA8WHJgzhIZf"},"source":["Make sure we shuffle the data here or BERT fails"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"prRQM8pDi8xI","colab":{}},"source":["df_trn = df_trn[[\"text\", \"label\"]]\n","df_tst = df_tst[[\"text\", \"label\"]]\n","\n","df_trn = df_trn.sample(frac=1)\n","df_tst = df_tst.sample(frac=1)\n","\n","df_trn.sample(n=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sfRnHSz3iSXz"},"source":["For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IuMOGwFui4it","colab":{}},"source":["DATA_COLUMN = 'text'\n","LABEL_COLUMN = 'label'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [1, 3, 5]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"V399W0rqNJ-Z"},"source":["# 2.0 Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves three steps:\n","- First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","- Second, we load the model specific tokenizer\n","- Third, we convert examples to features BERT understands using parts 1 and 2"]},{"cell_type":"markdown","metadata":{"id":"sBhP3uyY1Hbt","colab_type":"text"},"source":["# 2.1 Input Features\n","\n","These classes are super simple and litterally just have 4 attributes that hold the text. What info they hold is listed below.\n","\n","\n","- `text_a` is the text we want to classify, which in this case, is the `text` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p9gEt5SmM6i6","outputId":"ce567300-4ed2-43a5-efea-bcf7c174ac98","executionInfo":{"status":"error","timestamp":1561768886660,"user_tz":180,"elapsed":530,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":251}},"source":["train_InputExamples = df_trn.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = df_tst.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-7fc6d6563010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_InputExamples = df_trn.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                    \u001b[0mtext_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATA_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                    \u001b[0mtext_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                    label = x[LABEL_COLUMN]), axis = 1)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_trn' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ktv2USB91Hbx","colab_type":"text"},"source":["## Understanding what BERT eats\n","\n","Now this is an ok explination to operate BERT basically, but if we want to really understand what is going inside we need to fully approciate theoretically what BERT wants for its inputs (this part is key for lots of later discussion to). And BERTs input representation is not intuitive (in my opinion).\n","\n","First we can mention how it wants its inputs ordered. \n","\n","It can take up to 512 tokens in a single 'sequence'. A sequence is defined as one input example (however that may be constructed). Note this limit on 512 is actually very important when we come to look at question answering.\n","\n","Said sequence is composed in one of two ways as best described by comments in the oficial repo...\n","\n","The convention in BERT is:\n","\n","(a) For sequence pairs:\n","\n","tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","\n","(b) For single sequences:\n","\n","tokens:   [CLS] the dog is hairy . [SEP]\n","\n","So depending on if we're doing single sequence or sequence to sequence we arrange the data in one of these two ways. Note BERT refer to the whole input as the sequence, and each of the two potential parts as sentences even though they can be longer or shorter than a linguistic sentence.\n","\n","Next we have to understand what BERT does with these words, goes to word embeddings? We'll yes but not so simply...\n","\n","![image.png](attachment:image.png)\n","\n","BERT actually sums 3 different embeddings, however as we shall see we only actually need to create the ids for the word embeddings and the segment embeddings as inputs to the model. Creating the positional embeddings, gettings the vectors, and summing is all done internally, and in this notebook we're not messing with core BERT."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SCZWZtKxObjh"},"source":["# 2.1 Tokenizer\n","\n","Next, we need to preprocess our data so that it matches the data BERT was trained on.This follows the following process.\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Now the original blog says \"Happily, we don't have to worry about most of these details.\", and simply instanciates an instance of the bert.tokenization.FullTokenizer class... but I want to go deeper.\n","\n","So lets look at BERTs tokenizer setup:"]},{"cell_type":"markdown","metadata":{"id":"6D1lYz8q1Hbz","colab_type":"text"},"source":["```\n","######################### SOURCE CODE ###############################\n","class FullTokenizer(object):\n","  \"\"\"Runs end-to-end tokenziation.\"\"\"\n","\n","  def __init__(self, vocab_file, do_lower_case=True):\n","    self.vocab = load_vocab(vocab_file)\n","    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n","    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n","\n","  def tokenize(self, text):\n","    split_tokens = []\n","    for token in self.basic_tokenizer.tokenize(text):\n","      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n","        split_tokens.append(sub_token)\n","\n","    return split_tokens\n","\n","  def convert_tokens_to_ids(self, tokens):\n","    return convert_by_vocab(self.vocab, tokens)\n","\n","  def convert_ids_to_tokens(self, ids):\n","    return convert_by_vocab(self.inv_vocab, ids)\n","######################### SOURCE CODE ###############################\n","```"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qMWiDtpyQSoU"},"source":["This above code is taken directly from their repository, as we can see the tokenizer has all the basic components we'd expect. \n","\n","- vocab is a dictionary that maps ids to words in the input vocab (familiar to anyone acustomed to working with language models)\n","- inv_vocab is the oposite, words to ids\n","- they then have two tokenizers, one is a more basic form and the other splits out word parts, they could have grouped the two\n","\n","Now this is simple enoguh, however the \"recomended\" way of instanciating this is (to me) super confusing!!! (See below)"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1564496829566,"user_tz":180,"elapsed":299839,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"IhJSe0QHNG7U","outputId":"9f8341ec-d055-4949-dd2a-79ce1478ea05","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","\n","\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                              tokenization_info[\"do_lower_case\"]])\n","        \n","tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n","                                            do_lower_case=do_lower_case)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["W0730 14:27:07.960558 140708289415040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eI4HtuQWriH6","colab_type":"code","outputId":"af84e1d6-6820-4bff-c745-678967d6b6ea","executionInfo":{"status":"ok","timestamp":1564496829568,"user_tz":180,"elapsed":297386,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["\n","f = open(vocab_file, \"r\")\n","print(len(f.read()))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["228209\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nU6tPStDR5Zh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1QAGT-K1Hb3","colab_type":"text"},"source":["So... what happened?\n","\n","Well here we are using Tensorflow Hub, which is Tensorflows newest way of easily sharing pre-trained models, its vital to understand this for anyone doing transfer learning in TF. The BERT model was loaded in the deafult graph (standard Tensorflow) from Hub https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1 by calling a set filepath into hub.Module. As such to go any further we really need to understand exactly what the hub.Module returns, and how we get at the underlying module. To start the official docs https://www.tensorflow.org/hub/api_docs/python/hub/Module are actually quite good on Module.__init__:\n","\n","######################### OFFICIAL DOCS ###############################\n","\n","Constructs a Module to be used in the current graph.\n","\n","This creates the module state-graph under an unused variable_scope based on name. During this call a Module will:\n","\n","Add GLOBAL_VARIABLES under its scope. Those variables may be added to to the TRAINABLE_VARIABLES collection (depending on trainable parameter) and to the MODEL_VARIABLES. The variables must be initialized before use, and can be checkpointed as usual.\n","\n","Add ops to the INIT_TABLE_OPS collection, which must be run during session initialization and add constant tensors to ASSET_FILEPATHS that are needed during the execution of such ops.\n","\n","Add tensors to the REGULARIZATION_LOSSES collection (depending on trainable parameter).\n","\n","Args:\n","- spec: A ModuleSpec defining the Module to instantiate or a path where to load a ModuleSpec from via load_module_spec.\n","- trainable: whether the Module is trainable. If False, no variables are added to TRAINABLE_VARIABLES collection, and no tensors are added to REGULARIZATION_LOSSES collection.\n","- name: A string, the variable scope name under which to create the Module. It will be uniquified and the equivalent name scope must be unused.\n","- tags: A set of strings specifying the graph variant to use.\n","\n","######################### OFFICIAL DOCS ###############################\n","\n","Thats reasonably clear (for tensorflow) we now have all of the variables of the BERT graph defined under a specific variable scope. It also added these to some dictionaries of what variables exist (for example trainable parameters). Finally it adds the operations to the INIT_TABLE_OPS.\n","\n","So now I have at least 3 questions (maybe inquisitive you has many more :))... \n","- How do we access the variables? Can we modify the graph? \n","- How do we call it? In what way is it being called?\n","- What in the world are these next 3 lines!?!?\n","\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    \n","    with tf.Session() as sess:\n","    \n","        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","                                            \n","To help with these im drawing a lot of inspiration from this tutorial that you should definetly check out https://medium.com/ymedialabs-innovation/how-to-use-tensorflow-hub-with-code-examples-9100edec29af.\n","\n","So to start with once we have the instance it is called using a 'signature'. Whats a signature? Well the signatures are pre-defined use cases for how the creator wants the user to call their model. We can see the available signatures by running:"]},{"cell_type":"code","metadata":{"id":"i0cYqT871Hb3","colab_type":"code","outputId":"9113ecc9-17ed-4c90-9009-ceb00806fcc2","executionInfo":{"status":"ok","timestamp":1556646464627,"user_tz":180,"elapsed":1205,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["bert_module.get_signature_names()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tokens', 'tokenization_info', 'mlm']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"SiL6zt_Y1Hb6","colab_type":"text"},"source":["Well now we have these abstracted use cases, but what do we need to give to them? And what do they output? As is the case with BERT this is documented on tf hub, they only actually support the 'tokens' signature... (even though we're using the tokenization_info one here...), however we can check by running:"]},{"cell_type":"code","metadata":{"id":"FNWxM2rv1Hb7","colab_type":"code","outputId":"260a67a7-7f4a-47ef-9c8d-a7531253a271","executionInfo":{"status":"ok","timestamp":1556646465270,"user_tz":180,"elapsed":1478,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["print(bert_module.get_input_info_dict(signature='tokens'))\n","print(bert_module.get_output_info_dict(signature='tokens'))\n","\n","print(bert_module.get_input_info_dict(signature='tokenization_info'))\n","print(bert_module.get_output_info_dict(signature='tokenization_info'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n","{'sequence_output': <hub.ParsedTensorInfo shape=(?, ?, 768) dtype=float32 is_sparse=False>, 'pooled_output': <hub.ParsedTensorInfo shape=(?, 768) dtype=float32 is_sparse=False>}\n","{}\n","{'do_lower_case': <hub.ParsedTensorInfo shape=() dtype=bool is_sparse=False>, 'vocab_file': <hub.ParsedTensorInfo shape=() dtype=string is_sparse=False>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TCEPnSMg1Hb9","colab_type":"text"},"source":["So given what we know about what BERT eats and the output we'd expect from a language model, 'tokens' seems to make sense in what it does, that is the main running process. However thats section 3. Here lets look more at 'tokenization_info', 'tokenization_info' doesnt take any inputs (makes sense as the tokenization should depend only on the model and not on the inputs), and gives two outputs: the vocab file and if we should lower everything... makes sense.\n","\n","Now this business makes sense:\n","```\n","tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","with tf.Session() as sess:\n","    vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                        tokenization_info[\"do_lower_case\"]])\n","```                                           \n","As tokenization_info (the variable) here is a pointer to two tensors in the graph, each of which are called and their output returned, noramlly in session.run we'd pass the input as well but we already know for these specific endpoints there are no inputs. Now session.run will run the pretrained graph and these two endpoints will give us what we need.\n","\n","I havn't addressed the accessing and messing with the main graph yet, I think that makes more sense in the model section where as it turns out we need to reload the model from tf hub in the same way, so for now we'll continue just knowning how to call the model and what it returns."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z4oFkhpZBDKm"},"source":["So back to the original tutorial... \"we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:\""]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1561769111878,"user_tz":180,"elapsed":499,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"dsBo6RCtQmwx","outputId":"70c69194-4277-4374-ca30-9b1b54b303cd","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer!\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer',\n"," '!']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"10U_fR3EDJ4s","colab_type":"code","outputId":"001d8737-ddea-4898-f30f-d70193c6c386","executionInfo":{"status":"error","timestamp":1556646468682,"user_tz":180,"elapsed":1592,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["#tokenizer.vocab[\"U1f600\"]"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-cb364c0de9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"U1f600\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyError\u001b[0m: 'U1f600'"]}]},{"cell_type":"markdown","metadata":{"id":"jyMW3Cg41HcC","colab_type":"text"},"source":["Now to add a bit of \"whats going on\" we can easily parse the vocab attribute (that we saw earlier as part of the tokenizer) to a pandas df to view the vocabulary"]},{"cell_type":"code","metadata":{"id":"nFINp5oN1HcD","colab_type":"code","outputId":"b4ede404-c00f-4a40-c256-2eb86e5b2a31","executionInfo":{"status":"ok","timestamp":1556646469451,"user_tz":180,"elapsed":1678,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["vocab_df = pd.DataFrame(np.array([list(tokenizer.vocab.keys()), list(tokenizer.vocab.values())]).T, columns=[\"Vocab\", \"ID\"])\n","vocab_df.sample(n=10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Vocab</th>\n","      <th>ID</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24477</th>\n","      <td>bolshevik</td>\n","      <td>24477</td>\n","    </tr>\n","    <tr>\n","      <th>8028</th>\n","      <td>renaissance</td>\n","      <td>8028</td>\n","    </tr>\n","    <tr>\n","      <th>26132</th>\n","      <td>##vez</td>\n","      <td>26132</td>\n","    </tr>\n","    <tr>\n","      <th>24048</th>\n","      <td>tnt</td>\n","      <td>24048</td>\n","    </tr>\n","    <tr>\n","      <th>15190</th>\n","      <td>torso</td>\n","      <td>15190</td>\n","    </tr>\n","    <tr>\n","      <th>21443</th>\n","      <td>peanut</td>\n","      <td>21443</td>\n","    </tr>\n","    <tr>\n","      <th>16075</th>\n","      <td>mccoy</td>\n","      <td>16075</td>\n","    </tr>\n","    <tr>\n","      <th>14078</th>\n","      <td>132</td>\n","      <td>14078</td>\n","    </tr>\n","    <tr>\n","      <th>28145</th>\n","      <td>kunst</td>\n","      <td>28145</td>\n","    </tr>\n","    <tr>\n","      <th>27650</th>\n","      <td>ported</td>\n","      <td>27650</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Vocab     ID\n","24477    bolshevik  24477\n","8028   renaissance   8028\n","26132        ##vez  26132\n","24048          tnt  24048\n","15190        torso  15190\n","21443       peanut  21443\n","16075        mccoy  16075\n","14078          132  14078\n","28145        kunst  28145\n","27650       ported  27650"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"x2lldTi11HcG","colab_type":"code","outputId":"a9044667-7149-48f4-be5f-d996578394ac","executionInfo":{"status":"ok","timestamp":1556646471579,"user_tz":180,"elapsed":1739,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(bert_module.variable_map)\n","tokenization_info['do_lower_case']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'module_apply_tokenization_info/Const:0' shape=() dtype=bool>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0OEzfFIt6GIc"},"source":["# 2.3 Examples to Features\n","\n","Next.... Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands. Now we know that BERT is hoping for inputs of the form:"]},{"cell_type":"code","metadata":{"id":"QG3YCaqF1HcJ","colab_type":"code","outputId":"5b8cf890-27e1-4ebd-b8d3-3647def2a47c","executionInfo":{"status":"ok","timestamp":1556646485179,"user_tz":180,"elapsed":1415,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(bert_module.get_input_info_dict(signature='tokens'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eGZrLqdm1HcO","colab_type":"text"},"source":["But as is this is completely black box as to what is happening here! Let's look"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1556646491809,"user_tz":180,"elapsed":6848,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"LL5W8gEGRTAf","outputId":"d26e43f9-cc40-4852-bca1-1e0842202e62","colab":{"base_uri":"https://localhost:8080/","height":2485}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 1572\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.400015 139837559089024 run_classifier.py:774] Writing example 0 of 1572\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.404440 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.407813 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this location was a group ##on find . . . won ' t be back . the girl behind the counter was nice but even though i had the group ##on right in front of me , she had to read it several times and i had to explain to her what i was supposed to be getting with it . the red light therapy is a nice idea . . . but the room the bed was in was lacking . they should hire an ex ##ter ##mina ##tor , not leave sticky bug traps down with dead bugs all over the floor where people generally have bare feet . . . yu ##ck ! ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.410256 139837559089024 run_classifier.py:464] tokens: [CLS] this location was a group ##on find . . . won ' t be back . the girl behind the counter was nice but even though i had the group ##on right in front of me , she had to read it several times and i had to explain to her what i was supposed to be getting with it . the red light therapy is a nice idea . . . but the room the bed was in was lacking . they should hire an ex ##ter ##mina ##tor , not leave sticky bug traps down with dead bugs all over the floor where people generally have bare feet . . . yu ##ck ! ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 3295 2001 1037 2177 2239 2424 1012 1012 1012 2180 1005 1056 2022 2067 1012 1996 2611 2369 1996 4675 2001 3835 2021 2130 2295 1045 2018 1996 2177 2239 2157 1999 2392 1997 2033 1010 2016 2018 2000 3191 2009 2195 2335 1998 1045 2018 2000 4863 2000 2014 2054 1045 2001 4011 2000 2022 2893 2007 2009 1012 1996 2417 2422 7242 2003 1037 3835 2801 1012 1012 1012 2021 1996 2282 1996 2793 2001 1999 2001 11158 1012 2027 2323 10887 2019 4654 3334 22311 4263 1010 2025 2681 15875 11829 16735 2091 2007 2757 12883 2035 2058 1996 2723 2073 2111 3227 2031 6436 2519 1012 1012 1012 9805 3600 999 999 102 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.411423 139837559089024 run_classifier.py:465] input_ids: 101 2023 3295 2001 1037 2177 2239 2424 1012 1012 1012 2180 1005 1056 2022 2067 1012 1996 2611 2369 1996 4675 2001 3835 2021 2130 2295 1045 2018 1996 2177 2239 2157 1999 2392 1997 2033 1010 2016 2018 2000 3191 2009 2195 2335 1998 1045 2018 2000 4863 2000 2014 2054 1045 2001 4011 2000 2022 2893 2007 2009 1012 1996 2417 2422 7242 2003 1037 3835 2801 1012 1012 1012 2021 1996 2282 1996 2793 2001 1999 2001 11158 1012 2027 2323 10887 2019 4654 3334 22311 4263 1010 2025 2681 15875 11829 16735 2091 2007 2757 12883 2035 2058 1996 2723 2073 2111 3227 2031 6436 2519 1012 1012 1012 9805 3600 999 999 102 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.414112 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.416495 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.417809 139837559089024 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.419634 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.420932 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] good , but the salad was not as good as remembered . still think its a great spot in sc ##oot ##sdale ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.422178 139837559089024 run_classifier.py:464] tokens: [CLS] good , but the salad was not as good as remembered . still think its a great spot in sc ##oot ##sdale ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2204 1010 2021 1996 16521 2001 2025 2004 2204 2004 4622 1012 2145 2228 2049 1037 2307 3962 1999 8040 17206 15145 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.423602 139837559089024 run_classifier.py:465] input_ids: 101 2204 1010 2021 1996 16521 2001 2025 2004 2204 2004 4622 1012 2145 2228 2049 1037 2307 3962 1999 8040 17206 15145 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.424867 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.426008 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 3 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.427166 139837559089024 run_classifier.py:468] label: 3 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.431373 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.432664 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this review can be applied to any and all quick trip locations . i have , on many occasions , gone out of my way to stop in q ##t over other gas stations , because not only are they much much nice ##r , but they are very clean and offer a ton of gas station fare ( drinks , shakes , coffee ##s , juice ##s , snacks , etc ) . i really appreciate how quickly they work , with cash ##ier ' s that are able to take care of two customers at once . i usually ab ##hor using gas station bathrooms because , let ' s face it , we ' ve all been there and seen how put ##rid [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.433958 139837559089024 run_classifier.py:464] tokens: [CLS] this review can be applied to any and all quick trip locations . i have , on many occasions , gone out of my way to stop in q ##t over other gas stations , because not only are they much much nice ##r , but they are very clean and offer a ton of gas station fare ( drinks , shakes , coffee ##s , juice ##s , snacks , etc ) . i really appreciate how quickly they work , with cash ##ier ' s that are able to take care of two customers at once . i usually ab ##hor using gas station bathrooms because , let ' s face it , we ' ve all been there and seen how put ##rid [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 3319 2064 2022 4162 2000 2151 1998 2035 4248 4440 5269 1012 1045 2031 1010 2006 2116 6642 1010 2908 2041 1997 2026 2126 2000 2644 1999 1053 2102 2058 2060 3806 3703 1010 2138 2025 2069 2024 2027 2172 2172 3835 2099 1010 2021 2027 2024 2200 4550 1998 3749 1037 10228 1997 3806 2276 13258 1006 8974 1010 10854 1010 4157 2015 1010 10869 2015 1010 27962 1010 4385 1007 1012 1045 2428 9120 2129 2855 2027 2147 1010 2007 5356 3771 1005 1055 2008 2024 2583 2000 2202 2729 1997 2048 6304 2012 2320 1012 1045 2788 11113 16368 2478 3806 2276 28942 2138 1010 2292 1005 1055 2227 2009 1010 2057 1005 2310 2035 2042 2045 1998 2464 2129 2404 14615 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.435210 139837559089024 run_classifier.py:465] input_ids: 101 2023 3319 2064 2022 4162 2000 2151 1998 2035 4248 4440 5269 1012 1045 2031 1010 2006 2116 6642 1010 2908 2041 1997 2026 2126 2000 2644 1999 1053 2102 2058 2060 3806 3703 1010 2138 2025 2069 2024 2027 2172 2172 3835 2099 1010 2021 2027 2024 2200 4550 1998 3749 1037 10228 1997 3806 2276 13258 1006 8974 1010 10854 1010 4157 2015 1010 10869 2015 1010 27962 1010 4385 1007 1012 1045 2428 9120 2129 2855 2027 2147 1010 2007 5356 3771 1005 1055 2008 2024 2583 2000 2202 2729 1997 2048 6304 2012 2320 1012 1045 2788 11113 16368 2478 3806 2276 28942 2138 1010 2292 1005 1055 2227 2009 1010 2057 1005 2310 2035 2042 2045 1998 2464 2129 2404 14615 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.436494 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.437804 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 5 (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.439043 139837559089024 run_classifier.py:468] label: 5 (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.449503 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.450787 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i don ' t know why i waited so long to try this place when it is literally right around the corner from my house . i think its because i went to the one in ah ##wat ##uke ##e a long time ago and didn ' t care for the food . but i don ' t think they have the same owners since the websites are totally different . but anyway , on to the review . we ordered take ##out and i was very impressed with how nicely it was all packaged and how quickly it was ready . also , the woman on the phone was very friendly , easy to understand in spite of an accent , and patient in explaining [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.452197 139837559089024 run_classifier.py:464] tokens: [CLS] i don ' t know why i waited so long to try this place when it is literally right around the corner from my house . i think its because i went to the one in ah ##wat ##uke ##e a long time ago and didn ' t care for the food . but i don ' t think they have the same owners since the websites are totally different . but anyway , on to the review . we ordered take ##out and i was very impressed with how nicely it was all packaged and how quickly it was ready . also , the woman on the phone was very friendly , easy to understand in spite of an accent , and patient in explaining [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2123 1005 1056 2113 2339 1045 4741 2061 2146 2000 3046 2023 2173 2043 2009 2003 6719 2157 2105 1996 3420 2013 2026 2160 1012 1045 2228 2049 2138 1045 2253 2000 1996 2028 1999 6289 24281 15851 2063 1037 2146 2051 3283 1998 2134 1005 1056 2729 2005 1996 2833 1012 2021 1045 2123 1005 1056 2228 2027 2031 1996 2168 5608 2144 1996 11744 2024 6135 2367 1012 2021 4312 1010 2006 2000 1996 3319 1012 2057 3641 2202 5833 1998 1045 2001 2200 7622 2007 2129 19957 2009 2001 2035 21972 1998 2129 2855 2009 2001 3201 1012 2036 1010 1996 2450 2006 1996 3042 2001 2200 5379 1010 3733 2000 3305 1999 8741 1997 2019 9669 1010 1998 5776 1999 9990 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.453476 139837559089024 run_classifier.py:465] input_ids: 101 1045 2123 1005 1056 2113 2339 1045 4741 2061 2146 2000 3046 2023 2173 2043 2009 2003 6719 2157 2105 1996 3420 2013 2026 2160 1012 1045 2228 2049 2138 1045 2253 2000 1996 2028 1999 6289 24281 15851 2063 1037 2146 2051 3283 1998 2134 1005 1056 2729 2005 1996 2833 1012 2021 1045 2123 1005 1056 2228 2027 2031 1996 2168 5608 2144 1996 11744 2024 6135 2367 1012 2021 4312 1010 2006 2000 1996 3319 1012 2057 3641 2202 5833 1998 1045 2001 2200 7622 2007 2129 19957 2009 2001 2035 21972 1998 2129 2855 2009 2001 3201 1012 2036 1010 1996 2450 2006 1996 3042 2001 2200 5379 1010 3733 2000 3305 1999 8741 1997 2019 9669 1010 1998 5776 1999 9990 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.454800 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.456113 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 3 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.457401 139837559089024 run_classifier.py:468] label: 3 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.461480 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.462778 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] we used the ta ##co cart for my 30th birthday this past weekend and we came away very impressed . eduardo was the guy that came to our fiesta , and he was incredible . when he arrived , we experienced a ha ##bo ##ob and he immediately stepped in and helped bring in our decorations and furniture . he waited about 30 minutes until serving because we were unsure if the ha ##bo ##ob was going to attack again . my wife and i are both in the restaurant bi ##z and so were many of the guests , we all came away impressed with the food quality . next time we throw a party , we will use them again ! make sure to [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.464303 139837559089024 run_classifier.py:464] tokens: [CLS] we used the ta ##co cart for my 30th birthday this past weekend and we came away very impressed . eduardo was the guy that came to our fiesta , and he was incredible . when he arrived , we experienced a ha ##bo ##ob and he immediately stepped in and helped bring in our decorations and furniture . he waited about 30 minutes until serving because we were unsure if the ha ##bo ##ob was going to attack again . my wife and i are both in the restaurant bi ##z and so were many of the guests , we all came away impressed with the food quality . next time we throw a party , we will use them again ! make sure to [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2057 2109 1996 11937 3597 11122 2005 2026 13293 5798 2023 2627 5353 1998 2057 2234 2185 2200 7622 1012 14846 2001 1996 3124 2008 2234 2000 2256 24050 1010 1998 2002 2001 9788 1012 2043 2002 3369 1010 2057 5281 1037 5292 5092 16429 1998 2002 3202 3706 1999 1998 3271 3288 1999 2256 14529 1998 7390 1012 2002 4741 2055 2382 2781 2127 3529 2138 2057 2020 12422 2065 1996 5292 5092 16429 2001 2183 2000 2886 2153 1012 2026 2564 1998 1045 2024 2119 1999 1996 4825 12170 2480 1998 2061 2020 2116 1997 1996 6368 1010 2057 2035 2234 2185 7622 2007 1996 2833 3737 1012 2279 2051 2057 5466 1037 2283 1010 2057 2097 2224 2068 2153 999 2191 2469 2000 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.465621 139837559089024 run_classifier.py:465] input_ids: 101 2057 2109 1996 11937 3597 11122 2005 2026 13293 5798 2023 2627 5353 1998 2057 2234 2185 2200 7622 1012 14846 2001 1996 3124 2008 2234 2000 2256 24050 1010 1998 2002 2001 9788 1012 2043 2002 3369 1010 2057 5281 1037 5292 5092 16429 1998 2002 3202 3706 1999 1998 3271 3288 1999 2256 14529 1998 7390 1012 2002 4741 2055 2382 2781 2127 3529 2138 2057 2020 12422 2065 1996 5292 5092 16429 2001 2183 2000 2886 2153 1012 2026 2564 1998 1045 2024 2119 1999 1996 4825 12170 2480 1998 2061 2020 2116 1997 1996 6368 1010 2057 2035 2234 2185 7622 2007 1996 2833 3737 1012 2279 2051 2057 5466 1037 2283 1010 2057 2097 2224 2068 2153 999 2191 2469 2000 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.466933 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.468225 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 5 (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:05.469425 139837559089024 run_classifier.py:468] label: 5 (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 450\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.604889 139837559089024 run_classifier.py:774] Writing example 0 of 450\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.621703 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.624737 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i would give this place no stars but ye ##lp ! wants me to give at least one . i went here with my boyfriend last night for dinner and next to our awful bu ##cca di be ##pp ##o ( scott ##sdale ) experience , i have to say , this one is on my list of most terrible restaurant experiences . the service was so bad , we didn ' t even get to try the food ! true ye ##lp ##er that i am , i did my homework before suggesting that we come here for dinner . i saw all the reviews about the \" bad service \" , but i paid more attention to the \" cheap \" and \" delicious [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.628988 139837559089024 run_classifier.py:464] tokens: [CLS] i would give this place no stars but ye ##lp ! wants me to give at least one . i went here with my boyfriend last night for dinner and next to our awful bu ##cca di be ##pp ##o ( scott ##sdale ) experience , i have to say , this one is on my list of most terrible restaurant experiences . the service was so bad , we didn ' t even get to try the food ! true ye ##lp ##er that i am , i did my homework before suggesting that we come here for dinner . i saw all the reviews about the \" bad service \" , but i paid more attention to the \" cheap \" and \" delicious [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2052 2507 2023 2173 2053 3340 2021 6300 14277 999 4122 2033 2000 2507 2012 2560 2028 1012 1045 2253 2182 2007 2026 6898 2197 2305 2005 4596 1998 2279 2000 2256 9643 20934 16665 4487 2022 9397 2080 1006 3660 15145 1007 3325 1010 1045 2031 2000 2360 1010 2023 2028 2003 2006 2026 2862 1997 2087 6659 4825 6322 1012 1996 2326 2001 2061 2919 1010 2057 2134 1005 1056 2130 2131 2000 3046 1996 2833 999 2995 6300 14277 2121 2008 1045 2572 1010 1045 2106 2026 19453 2077 9104 2008 2057 2272 2182 2005 4596 1012 1045 2387 2035 1996 4391 2055 1996 1000 2919 2326 1000 1010 2021 1045 3825 2062 3086 2000 1996 1000 10036 1000 1998 1000 12090 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.631678 139837559089024 run_classifier.py:465] input_ids: 101 1045 2052 2507 2023 2173 2053 3340 2021 6300 14277 999 4122 2033 2000 2507 2012 2560 2028 1012 1045 2253 2182 2007 2026 6898 2197 2305 2005 4596 1998 2279 2000 2256 9643 20934 16665 4487 2022 9397 2080 1006 3660 15145 1007 3325 1010 1045 2031 2000 2360 1010 2023 2028 2003 2006 2026 2862 1997 2087 6659 4825 6322 1012 1996 2326 2001 2061 2919 1010 2057 2134 1005 1056 2130 2131 2000 3046 1996 2833 999 2995 6300 14277 2121 2008 1045 2572 1010 1045 2106 2026 19453 2077 9104 2008 2057 2272 2182 2005 4596 1012 1045 2387 2035 1996 4391 2055 1996 1000 2919 2326 1000 1010 2021 1045 3825 2062 3086 2000 1996 1000 10036 1000 1998 1000 12090 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.634519 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.638449 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.640321 139837559089024 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.645343 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.647080 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the staff at this location was rude and un ##fr ##ien ##dly ! ! also the store itself was not the clean ##est i have seen ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.649372 139837559089024 run_classifier.py:464] tokens: [CLS] the staff at this location was rude and un ##fr ##ien ##dly ! ! also the store itself was not the clean ##est i have seen ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3095 2012 2023 3295 2001 12726 1998 4895 19699 9013 18718 999 999 2036 1996 3573 2993 2001 2025 1996 4550 4355 1045 2031 2464 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.652951 139837559089024 run_classifier.py:465] input_ids: 101 1996 3095 2012 2023 3295 2001 12726 1998 4895 19699 9013 18718 999 999 2036 1996 3573 2993 2001 2025 1996 4550 4355 1045 2031 2464 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.655839 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.661016 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.663416 139837559089024 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.667406 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.672076 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] can ##t go wrong with a double double , fries , and a shake ! always good . great service . i got some cold fries once , called the corporate office , they sent out some gift cards for our meals , then followed up with a thank you card as well . not to mention they sent a coup ##on in a x ##mas card that year . you [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.673922 139837559089024 run_classifier.py:464] tokens: [CLS] can ##t go wrong with a double double , fries , and a shake ! always good . great service . i got some cold fries once , called the corporate office , they sent out some gift cards for our meals , then followed up with a thank you card as well . not to mention they sent a coup ##on in a x ##mas card that year . you [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2064 2102 2175 3308 2007 1037 3313 3313 1010 22201 1010 1998 1037 6073 999 2467 2204 1012 2307 2326 1012 1045 2288 2070 3147 22201 2320 1010 2170 1996 5971 2436 1010 2027 2741 2041 2070 5592 5329 2005 2256 12278 1010 2059 2628 2039 2007 1037 4067 2017 4003 2004 2092 1012 2025 2000 5254 2027 2741 1037 8648 2239 1999 1037 1060 9335 4003 2008 2095 1012 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.676099 139837559089024 run_classifier.py:465] input_ids: 101 2064 2102 2175 3308 2007 1037 3313 3313 1010 22201 1010 1998 1037 6073 999 2467 2204 1012 2307 2326 1012 1045 2288 2070 3147 22201 2320 1010 2170 1996 5971 2436 1010 2027 2741 2041 2070 5592 5329 2005 2256 12278 1010 2059 2628 2039 2007 1037 4067 2017 4003 2004 2092 1012 2025 2000 5254 2027 2741 1037 8648 2239 1999 1037 1060 9335 4003 2008 2095 1012 2017 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.678956 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.683132 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 5 (id = 2)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.686676 139837559089024 run_classifier.py:468] label: 5 (id = 2)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.705815 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.708166 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] great in its day , now leaves a lot to be desired . deserves credit as a phoenix landmark chinese restaurant . this restaurant has been here for 38 years . i ' ve been going here off and on for 35 years . i go here more for sentimental reasons , and not so much for the food . the interior still retains some of the original decoration style from the original restaurant established nearly 40 years ago . the rest of the decorations have been torn out . i believe the decor is modeled after the forbidden palace in beijing - see the vault - of - the - heavens style cr ##ene ##lation ##s in the ceiling . i have this restaurant - [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.712335 139837559089024 run_classifier.py:464] tokens: [CLS] great in its day , now leaves a lot to be desired . deserves credit as a phoenix landmark chinese restaurant . this restaurant has been here for 38 years . i ' ve been going here off and on for 35 years . i go here more for sentimental reasons , and not so much for the food . the interior still retains some of the original decoration style from the original restaurant established nearly 40 years ago . the rest of the decorations have been torn out . i believe the decor is modeled after the forbidden palace in beijing - see the vault - of - the - heavens style cr ##ene ##lation ##s in the ceiling . i have this restaurant - [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2307 1999 2049 2154 1010 2085 3727 1037 2843 2000 2022 9059 1012 17210 4923 2004 1037 6708 8637 2822 4825 1012 2023 4825 2038 2042 2182 2005 4229 2086 1012 1045 1005 2310 2042 2183 2182 2125 1998 2006 2005 3486 2086 1012 1045 2175 2182 2062 2005 23069 4436 1010 1998 2025 2061 2172 2005 1996 2833 1012 1996 4592 2145 14567 2070 1997 1996 2434 11446 2806 2013 1996 2434 4825 2511 3053 2871 2086 3283 1012 1996 2717 1997 1996 14529 2031 2042 7950 2041 1012 1045 2903 1996 25545 2003 14440 2044 1996 10386 4186 1999 7211 1011 2156 1996 11632 1011 1997 1011 1996 1011 17223 2806 13675 8625 13490 2015 1999 1996 5894 1012 1045 2031 2023 4825 1011 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.716689 139837559089024 run_classifier.py:465] input_ids: 101 2307 1999 2049 2154 1010 2085 3727 1037 2843 2000 2022 9059 1012 17210 4923 2004 1037 6708 8637 2822 4825 1012 2023 4825 2038 2042 2182 2005 4229 2086 1012 1045 1005 2310 2042 2183 2182 2125 1998 2006 2005 3486 2086 1012 1045 2175 2182 2062 2005 23069 4436 1010 1998 2025 2061 2172 2005 1996 2833 1012 1996 4592 2145 14567 2070 1997 1996 2434 11446 2806 2013 1996 2434 4825 2511 3053 2871 2086 3283 1012 1996 2717 1997 1996 14529 2031 2042 7950 2041 1012 1045 2903 1996 25545 2003 14440 2044 1996 10386 4186 1999 7211 1011 2156 1996 11632 1011 1997 1011 1996 1011 17223 2806 13675 8625 13490 2015 1999 1996 5894 1012 1045 2031 2023 4825 1011 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.720769 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.724273 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.727106 139837559089024 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.732068 139837559089024 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.736230 139837559089024 run_classifier.py:462] guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] we got the wise ##gu ##y with wood roasted onion , house smoked mo ##zza ##rella , and fen ##nel sausage . i was very under ##w ##helm ##ed . the pizza was really greasy and just not all that great - tasting . also the crust was burnt in several places , making it taste even worse . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.738482 139837559089024 run_classifier.py:464] tokens: [CLS] we got the wise ##gu ##y with wood roasted onion , house smoked mo ##zza ##rella , and fen ##nel sausage . i was very under ##w ##helm ##ed . the pizza was really greasy and just not all that great - tasting . also the crust was burnt in several places , making it taste even worse . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2057 2288 1996 7968 12193 2100 2007 3536 28115 20949 1010 2160 20482 9587 20715 21835 1010 1998 21713 11877 24165 1012 1045 2001 2200 2104 2860 24546 2098 1012 1996 10733 2001 2428 26484 1998 2074 2025 2035 2008 2307 1011 18767 1012 2036 1996 19116 2001 11060 1999 2195 3182 1010 2437 2009 5510 2130 4788 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.742577 139837559089024 run_classifier.py:465] input_ids: 101 2057 2288 1996 7968 12193 2100 2007 3536 28115 20949 1010 2160 20482 9587 20715 21835 1010 1998 21713 11877 24165 1012 1045 2001 2200 2104 2860 24546 2098 1012 1996 10733 2001 2428 26484 1998 2074 2025 2035 2008 2307 1011 18767 1012 2036 1996 19116 2001 11060 1999 2195 3182 1010 2437 2009 5510 2130 4788 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.745273 139837559089024 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.748325 139837559089024 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 3 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["I0430 17:48:09.752240 139837559089024 run_classifier.py:468] label: 3 (id = 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nWuSCvDf2RDb","colab_type":"text"},"source":["Right.... lets have a look at exactly what happened there by again looking at the source code."]},{"cell_type":"markdown","metadata":{"id":"kFXtaWJZ1HcT","colab_type":"text"},"source":["######################### SOURCE CODE ###############################\n","```\n","# This function is not used by this file but is still used by the Colab and\n","# people who depend on it.\n","def convert_examples_to_features(examples, label_list, max_seq_length,\n","                                 tokenizer):\n","  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n","\n","  features = []\n","  for (ex_index, example) in enumerate(examples):\n","    if ex_index % 10000 == 0:\n","      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","    feature = convert_single_example(ex_index, example, label_list,\n","                                     max_seq_length, tokenizer)\n","\n","    features.append(feature)\n","  return features\n","```\n","######################### SOURCE CODE ###############################"]},{"cell_type":"markdown","metadata":{"id":"IrYccyBP1HcU","colab_type":"text"},"source":["So lets have a look whats happening here...\n","Inputs:\n","- examples - it is taking as input our list of classes as \"examples\", where each class just has 4 arributes that hold the relevant info (guid, text_a, text_b, label)\n","- label_list - our python list with the values of the labels for our classification task\n","- max_sequence_length - Integer, a hyperparameter of how many steps (word parts) we want to analyse, common hyperparameter for those of us familiar with language modelling\n","- tokenizer -  the custom tokenizer class that holds the vocab and methods for tokenizing text.\n","\n","So whats the method doing? Well its simply looping through each example (doing a bit of logging) and then appending a simple converted example to a python list before returning the list. This single example depends on all the things above but now also has an index. Lets look at the source code for convert_single_example..."]},{"cell_type":"markdown","metadata":{"id":"tA_JNt5o1HcU","colab_type":"text"},"source":["######################### SOURCE CODE ###############################\n","```\n","def convert_single_example(ex_index, example, label_list, max_seq_length,\n","                           tokenizer):\n","  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","  if isinstance(example, PaddingInputExample):\n","    return InputFeatures(\n","        input_ids=[0] * max_seq_length,\n","        input_mask=[0] * max_seq_length,\n","        segment_ids=[0] * max_seq_length,\n","        label_id=0,\n","        is_real_example=False)\n","\n","  label_map = {}\n","  for (i, label) in enumerate(label_list):\n","    label_map[label] = i\n","\n","  tokens_a = tokenizer.tokenize(example.text_a)\n","  tokens_b = None\n","  if example.text_b:\n","    tokens_b = tokenizer.tokenize(example.text_b)\n","\n","  if tokens_b:\n","    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","    # length is less than the specified length.\n","    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","  else:\n","    # Account for [CLS] and [SEP] with \"- 2\"\n","    if len(tokens_a) > max_seq_length - 2:\n","      tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","  # The convention in BERT is:\n","  # (a) For sequence pairs:\n","  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","  # (b) For single sequences:\n","  #  tokens:   [CLS] the dog is hairy . [SEP]\n","  #  type_ids: 0     0   0   0  0     0 0\n","  #\n","  # Where \"type_ids\" are used to indicate whether this is the first\n","  # sequence or the second sequence. The embedding vectors for `type=0` and\n","  # `type=1` were learned during pre-training and are added to the wordpiece\n","  # embedding vector (and position vector). This is not *strictly* necessary\n","  # since the [SEP] token unambiguously separates the sequences, but it makes\n","  # it easier for the model to learn the concept of sequences.\n","  #\n","  # For classification tasks, the first vector (corresponding to [CLS]) is\n","  # used as the \"sentence vector\". Note that this only makes sense because\n","  # the entire model is fine-tuned.\n","  tokens = []\n","  segment_ids = []\n","  tokens.append(\"[CLS]\")\n","  segment_ids.append(0)\n","  for token in tokens_a:\n","    tokens.append(token)\n","    segment_ids.append(0)\n","  tokens.append(\"[SEP]\")\n","  segment_ids.append(0)\n","\n","  if tokens_b:\n","    for token in tokens_b:\n","      tokens.append(token)\n","      segment_ids.append(1)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(1)\n","\n","  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","  # tokens are attended to.\n","  input_mask = [1] * len(input_ids)\n","\n","  # Zero-pad up to the sequence length.\n","  while len(input_ids) < max_seq_length:\n","    input_ids.append(0)\n","    input_mask.append(0)\n","    segment_ids.append(0)\n","\n","  assert len(input_ids) == max_seq_length\n","  assert len(input_mask) == max_seq_length\n","  assert len(segment_ids) == max_seq_length\n","\n","  label_id = label_map[example.label]\n","  if ex_index < 5:\n","    tf.logging.info(\"*** Example ***\")\n","    tf.logging.info(\"guid: %s\" % (example.guid))\n","    tf.logging.info(\"tokens: %s\" % \" \".join(\n","        [tokenization.printable_text(x) for x in tokens]))\n","    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n","\n","  feature = InputFeatures(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids,\n","      label_id=label_id,\n","      is_real_example=True)\n","  return feature\n","```\n","######################### SOURCE CODE ###############################"]},{"cell_type":"markdown","metadata":{"id":"a-YwzZ3-1HcV","colab_type":"text"},"source":["This is a bit more involved (i.e. long) but its actually quite transparent what they're doing if we break it down. So lets start by remembering what we mentioned earlier around how the BERT model want to read its inputs. BERT wants inputs in the form:\n","\n","(a) For sequence pairs:\n","\n","tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","\n","(b) For single sequences:\n","\n","tokens:   [CLS] the dog is hairy . [SEP]\n","\n","So thats what the code above is doing:\n","1. It has some house keeping to check for fake examples (we'll ignore that for now)\n","2. It tokenizes text_a and if it exists text_b\n","3. Depending on if text_b exists it limits the combined 'sequence' to the max_sequence_length\n","4. Then it creates 2 lists\n","    - The list of tokens with special tokens [SEP] etc\n","    - The list of type ids which is 0 for text_a and 0 for text_b\n","5. Next it converts the tokenized text to ids (which the language model needs to run)\n","6. Then it creates a new list, and input mask so it knows which of the tokens out of max_sequence_length are actual data and not just fillers\n","7. It pads our 3 lists: word_ids, type_ids, and input_mask; out to max_sequence_length with 0s\n","8. Then it passes these 3 lists with the label_id (which it made a mapping of earlier) to a new constructor called InputFeatures\n","\n","Pretty straight forward, even if we are diving a little deep. Lets look at InputFeatures"]},{"cell_type":"markdown","metadata":{"id":"vJalJJ7d1HcW","colab_type":"text"},"source":["######################### SOURCE CODE ###############################\n","```\n","class InputFeatures(object):\n","  \"\"\"A single set of features of data.\"\"\"\n","\n","  def __init__(self,\n","               input_ids,\n","               input_mask,\n","               segment_ids,\n","               label_id,\n","               is_real_example=True):\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.label_id = label_id\n","    self.is_real_example = is_real_example\n","```\n","######################### SOURCE CODE ###############################"]},{"cell_type":"markdown","metadata":{"id":"F-AxdIXs1HcX","colab_type":"text"},"source":["Aaaaaaand InputFeatures is just a class to help hold all of the info.... so we're done!\n","\n","Note these positional embeddings that BERT talks about dont enter into our inputs, those and the summing of the vectors is part of the inner workings of BERT that we havn't got to yet."]},{"cell_type":"code","metadata":{"id":"9lT4sPAE1HcZ","colab_type":"code","outputId":"4f685ff1-d8fd-47e2-ba0f-df3ddde2fd28","executionInfo":{"status":"ok","timestamp":1556646491812,"user_tz":180,"elapsed":944,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(bert_module.get_input_info_dict(signature='tokens'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e3sOkuVa1Hcb","colab_type":"code","outputId":"c414cec4-3aaf-4f27-9045-9a67e2d8953c","executionInfo":{"status":"ok","timestamp":1556646492372,"user_tz":180,"elapsed":815,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["print(\"Length of training inputs matches with the original size of our dataset nicely\", len(train_features))\n","print(\"And we can see the components that make up each training example, for example the vocab_ids of example 1:\\n\", train_features[0].input_ids)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Length of training inputs matches with the original size of our dataset nicely 1572\n","And we can see the components that make up each training example, for example the vocab_ids of example 1:\n"," [101, 2023, 3295, 2001, 1037, 2177, 2239, 2424, 1012, 1012, 1012, 2180, 1005, 1056, 2022, 2067, 1012, 1996, 2611, 2369, 1996, 4675, 2001, 3835, 2021, 2130, 2295, 1045, 2018, 1996, 2177, 2239, 2157, 1999, 2392, 1997, 2033, 1010, 2016, 2018, 2000, 3191, 2009, 2195, 2335, 1998, 1045, 2018, 2000, 4863, 2000, 2014, 2054, 1045, 2001, 4011, 2000, 2022, 2893, 2007, 2009, 1012, 1996, 2417, 2422, 7242, 2003, 1037, 3835, 2801, 1012, 1012, 1012, 2021, 1996, 2282, 1996, 2793, 2001, 1999, 2001, 11158, 1012, 2027, 2323, 10887, 2019, 4654, 3334, 22311, 4263, 1010, 2025, 2681, 15875, 11829, 16735, 2091, 2007, 2757, 12883, 2035, 2058, 1996, 2723, 2073, 2111, 3227, 2031, 6436, 2519, 1012, 1012, 1012, 9805, 3600, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2wC4EWcJ1Hcd","colab_type":"text"},"source":["Note the 'train_features' are still python objects (lists etc) and BERT wants tensors, however how we parse in tensors is actually dealt with as part of the model in the next section."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ccp5trMwRtmr"},"source":["# 3.0 Creating a model\n","\n","Now we have our data in a form BERT can hopefully understand (it's still lacking the conversion to tesnors) we can build out a bigger model. \n","\n","But first a bit of terminology. Whoever attended mine and Steve's talk on transfer learning heard us talk about a 3 stage approach: train, fine-tune, classification. This was based on ULMFiT, here a similar aproach is used but without doing stage 2... and confusingly refering to our stage 3 as 'fine-tuning' so just watch out when you hear these terms. It is important to understand exactly what we're doing. Here as we're looking at BERT we'll go with their terminaology, training TL classifiers is 'fine-tuning'.\n","\n","Now the workflow standard TF is to use estimators, that is the example given officially and they do make things \"easier\" once you understand whats going on... the hard part is understanding whats going on. Estimators are high-level apis for dealing with the loading of data into a model and the training of the model. I shall give a quick overview here highlighting where BERT black box code comes in and how it links to the general case. For a more complete overview please look here https://www.youtube.com/watch?v=BhQW2OLzx_c and on the official TF site https://www.tensorflow.org/guide/estimators.\n","\n","In general the work flow is shown in this chart.\n","\n","![image.png](attachment:image.png)\n","\n","To run through quickly we want to call the train method (actually there are other methods like predict etc we will want to call later, but they all work in the same way so for here ill stick with training) on an estimator. This needs a train_input_fn which deals with data loading. We also have to initilise the estimator object which involves creating a config instace, and a model_fn, the model_fn is made by a model_fn creator which calls a create_model function...\n","\n","So to simplify this down and understand this we really need to build 3 things:\n","- An instance of tf.estimator.RunConfig\n","- A train_input_fn which is a specific kind of function needed to run the estimator to train the data\n","- A model_fn which the estimator wraps around and \"talks to\" the train_input_function\n","\n","There are couple of \"function builders\" but they are just wrappers that we dont absolutely need..."]},{"cell_type":"markdown","metadata":{"id":"qVL_aeLS1Hce","colab_type":"text"},"source":["# 3.1 tf.estimator.RunConfig\n","\n","We start with this part not becasue its the most logical to start with, but because its the easiest! We can get this out of the way quickly, as shown below all we need to do is define a config class on how and where we save the model as it trains."]},{"cell_type":"code","metadata":{"id":"Fw1JZjsD1Hcf","colab_type":"code","colab":{}},"source":["# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100\n","\n","# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=\"model\",\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTbbm4SP1Hch","colab_type":"text"},"source":["# 3.2 train_input_fn\n","\n","Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. The estimator we will build needs this, according to the official estimators docmentation this function needs to...\n","\n","\"A function that provides input data for training as minibatches. See Premade Estimators for more information. The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below. * A tuple (features, labels): Where features is a tf.Tensor or a dictionary of string feature name to Tensor and labels is a Tensor or a dictionary of string label name to Tensor. Both features and labels are consumed by model_fn. They should satisfy the expectation of model_fn from inputs.\"\n","\n","Now it appears BERT has another black box creator for us, but we can pull that apart and see whats happening."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1Pv2bAlOX_-K","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-46yAc71Hcl","colab_type":"text"},"source":["Now looking at the BERT code bert.run_classifier.input_fn_builder"]},{"cell_type":"markdown","metadata":{"id":"zNV3rZo61Hcl","colab_type":"text"},"source":["######################### SOURCE CODE ###############################\n","```\n","def input_fn_builder(features, seq_length, is_training, drop_remainder):\n","  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n","\n","  all_input_ids = []\n","  all_input_mask = []\n","  all_segment_ids = []\n","  all_label_ids = []\n","\n","  for feature in features:\n","    all_input_ids.append(feature.input_ids)\n","    all_input_mask.append(feature.input_mask)\n","    all_segment_ids.append(feature.segment_ids)\n","    all_label_ids.append(feature.label_id)\n","\n","  def input_fn(params):\n","    \"\"\"The actual input function.\"\"\"\n","    batch_size = params[\"batch_size\"]\n","\n","    num_examples = len(features)\n","\n","    # This is for demo purposes and does NOT scale to large data sets. We do\n","    # not use Dataset.from_generator() because that uses tf.py_func which is\n","    # not TPU compatible. The right way to load data is with TFRecordReader.\n","    d = tf.data.Dataset.from_tensor_slices({\n","        \"input_ids\":\n","            tf.constant(\n","                all_input_ids, shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"input_mask\":\n","            tf.constant(\n","                all_input_mask,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"segment_ids\":\n","            tf.constant(\n","                all_segment_ids,\n","                shape=[num_examples, seq_length],\n","                dtype=tf.int32),\n","        \"label_ids\":\n","            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n","    })\n","\n","    if is_training:\n","      d = d.repeat()\n","      d = d.shuffle(buffer_size=100)\n","\n","    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n","    return d\n","\n","  return input_fn\n","```\n","######################### SOURCE CODE ###############################"]},{"cell_type":"markdown","metadata":{"id":"7yhi4n1o1Hcm","colab_type":"text"},"source":["We can see that the BERT function builder is doing what estimators expects, its building and returning a shuffled, repeated, batch of data (these things could be tweaked depending on the project if we went in manually) of batch_size. With all of the components that we need in the form of tf.data.Dataset. We can also see these 4 components match exactly the attribute names of the objects returned in train_features as given by the InputFeatures class.\n","\n","We also know this should talk to our model function (even though we've not made it yet) as we know from the TF Hub that we expect 3 of these 4 inputs in the form of tensors, the label_ids is the extra bit for our classification task.\n","\n","We can see how the train_input_fn works if we pass some dummy params"]},{"cell_type":"code","metadata":{"id":"rDT-FCpr1Hcm","colab_type":"code","outputId":"64b0a993-035b-489e-a6b0-0b757121832d","executionInfo":{"status":"ok","timestamp":1556646502019,"user_tz":180,"elapsed":2183,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_input_fn({'batch_size': 32})"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<DatasetV1Adapter shapes: {input_ids: (?, 128), input_mask: (?, 128), segment_ids: (?, 128), label_ids: (?,)}, types: {input_ids: tf.int32, input_mask: tf.int32, segment_ids: tf.int32, label_ids: tf.int32}>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"ts5tjCQ71Hcp","colab_type":"code","outputId":"23b88cb7-2bb2-46b0-a9ac-d8ce9e79d45d","executionInfo":{"status":"ok","timestamp":1556646502021,"user_tz":180,"elapsed":1936,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["bert_module.get_input_info_dict(signature='tokens')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>,\n"," 'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>,\n"," 'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"WhwCmBjS1Hcs","colab_type":"text"},"source":["# 3.3 model_fn\n","\n","Finally we arrive at the juciy bit, the model! this is the bit where we really have to disect the BERT black box code to understand fully whats going on and use this in the most agnostic manner. Now we already know we need to use TF Hub to get the model and weights, we also know we need it to take in at least 3 inputs to satisfy the model (we are actually passing 4 because we need our label_id). However ho the training is managed we still need to investigate.\n","\n","Below is the model from the official tutorial, but I have added some annotations.."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6o2a5ZIvRcJq","colab":{}},"source":["def create_model(is_predicting, #Bool to say if we're predicting (or training) as the output we'd like from the model would move\n","                 input_ids, # We know that our train_input_fn will pass the inputs as tensor of mini_batch_size, they\n","                 #actually come as one dictionary but in the next cell we see that in model_fn we split them out\n","                 input_mask, \n","                 segment_ids, \n","                 labels,\n","                 num_labels): # Num labels just lets us define the size of our softmax level for classification\n","    \"\"\"Creates a classification model.\"\"\"\n","\n","    # Get the BERT model again from tf hub, this time setting trainable as true as we will fine tune it\n","    bert_module = hub.Module(\n","      BERT_MODEL_HUB,\n","      trainable=True)\n","    \n","    # We have our input tensors, the ones for the pre-trained bert are just being grouped here for convinience\n","    bert_inputs = dict(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids)\n","    \n","    # We already saw earlier in the notebook, 'tokens' is the standard endpoint that gives the reference tensors coming out of BERT\n","    # Unlike 'tokenization_info' here we need to pass the inputs to get the outputs.\n","    bert_outputs = bert_module(\n","      inputs=bert_inputs,\n","      signature=\"tokens\",\n","      as_dict=True)\n","\n","    # From the tf hub docs for BERT we know we can use \"pooled_output\" for classification tasks on an entire sentence.\n","    # Or use \"sequence_outputs\" for token-level output.\n","    output_layer = bert_outputs[\"pooled_output\"]\n","\n","    hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own classification layer\n","    output_weights = tf.get_variable(\n","      \"output_weights\", [num_labels, hidden_size],\n","      initializer=tf.truncated_normal_initializer(stddev=0.02))\n","\n","    output_bias = tf.get_variable(\n","      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","    with tf.variable_scope(\"loss\"):\n","\n","        # Dropout helps prevent overfitting\n","        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","        \n","        print(output_layer.shape)\n","        print([batch_size * seq_length, hidden_size])\n","\n","        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","        logits = tf.nn.bias_add(logits, output_bias)\n","        log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","        # Convert labels into one-hot encoding\n","        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","        # If we're predicting, we want predicted labels and the probabiltiies.\n","        if is_predicting:\n","              return (predicted_labels, log_probs)\n","\n","        # If we're train/eval, compute loss between predicted and actual label\n","        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","        loss = tf.reduce_mean(per_example_loss)\n","        return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qpE0ZIDOCQzE"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction. Remeber its the actual 'model_fn' we want, the rest is extra abstraction commonly used to make things \"easier\"."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FnH-AnOQ9KKW","colab":{}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, \n","                     learning_rate, \n","                     num_train_steps,\n","                     num_warmup_steps):\n","    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","\n","    def model_fn(features, \n","                 labels, \n","                 mode, \n","                 params): \n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","\n","        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","\n","        # TRAIN and EVAL\n","        if not is_predicting:\n","\n","            # Here we run our model and return our loss function and friends\n","            (loss, predicted_labels, log_probs) = create_model(\n","            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            # Here we define a training op, this one comes from BERT we will open this up in a second and have a look whats going on\n","            # but logically it makes sense what its doing, actually running the training\n","            train_op = bert.optimization.create_optimizer(\n","              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","            # Calculate evaluation metrics and call them. This is a little subtle, the types of metrics we can use are limited to\n","            # tf's tf.metrics library which is definetly not exaustive (no multiclass f1 score...), as such what we can actually \n","            # do is largely ignore this and calculate the metrics in python with the returned probabilites on the test set\n","            def metric_fn(label_ids, predicted_labels):\n","                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","                return {\"eval_accuracy\": accuracy}\n","\n","            eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","            # Next we create a different end point for model_fn depending on what we're doing (training, predicting, etc)\n","            # All of these are instances of tf.estimator.EstimatorSpec which we will unpack below\n","\n","            if mode == tf.estimator.ModeKeys.TRAIN:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                    loss=loss,\n","                    train_op=train_op)\n","            else:\n","                return tf.estimator.EstimatorSpec(mode=mode,\n","                    loss=loss,\n","                    eval_metric_ops=eval_metrics)\n","        else:\n","            (predicted_labels, log_probs) = create_model(\n","               is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","            predictions = {\n","              'probabilities': log_probs,\n","              'labels': predicted_labels\n","            }\n","            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","    # Return the actual model function in the closure\n","    return model_fn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19zOtAYI1Hc0","colab_type":"text"},"source":["So the model_fn_builder and the model_fn are pretty clear with a couple of key things we'd like to deep-dive. \n","\n","- whats making our train_op inside bert.optimization.create_optimizer\n","- whats this: tf.estimator.EstimatorSpec? And how do we make one for an arbitrary model_fn going into an estimator\n","\n","First lets look at bert.optimization.create_optimizer"]},{"cell_type":"markdown","metadata":{"id":"w6A8IQke1Hc3","colab_type":"text"},"source":["## bert.optimization.create_optimizer\n","\n","######################### SOURCE CODE ###############################\n","```\n","def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n","  \"\"\"Creates an optimizer training op.\"\"\"\n","  global_step = tf.train.get_or_create_global_step()\n","\n","  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n","\n","  # Implements linear decay of the learning rate.\n","  learning_rate = tf.train.polynomial_decay(\n","      learning_rate,\n","      global_step,\n","      num_train_steps,\n","      end_learning_rate=0.0,\n","      power=1.0,\n","      cycle=False)\n","\n","  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n","  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n","  if num_warmup_steps:\n","    global_steps_int = tf.cast(global_step, tf.int32)\n","    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n","\n","    global_steps_float = tf.cast(global_steps_int, tf.float32)\n","    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n","\n","    warmup_percent_done = global_steps_float / warmup_steps_float\n","    warmup_learning_rate = init_lr * warmup_percent_done\n","\n","    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n","    learning_rate = (\n","        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n","\n","  # It is recommended that you use this optimizer for fine tuning, since this\n","  # is how the model was trained (note that the Adam m/v variables are NOT\n","  # loaded from init_checkpoint.)\n","  optimizer = AdamWeightDecayOptimizer(\n","      learning_rate=learning_rate,\n","      weight_decay_rate=0.01,\n","      beta_1=0.9,\n","      beta_2=0.999,\n","      epsilon=1e-6,\n","      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n","```\n","######################### SOURCE CODE ###############################\n","\n","This is all quite straightforward, they are just defining an Adam optimizer with a linear learning rate decay. In addition they have the option of \"warmup\" which is the same logic FastAI used with ULMFiT in have slantend triangular learning rates, the idea is that you start low, build up to your max learning rate, and then linearly decrease.\n","\n","## tf.estimator.EstimatorSpec\n","\n","This is a standard class and is quite straightforward. As per the docs https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec the EstimatorSpec is what is to be returned from any model  function and has various attributes around the mode, loss, train_op, etc and these must \"fully define\" the model to be run by the estimator. So the model function is just defineing the model and filling in this class object with the relevant tf.variables to be ran."]},{"cell_type":"markdown","metadata":{"id":"eCJ9so_U1Hc4","colab_type":"text"},"source":["Now we have all of the (confusing) constituent pieces, we can put them together"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OjwJ4bTeWXD8","outputId":"8e517a3e-a409-4c64-92b2-431c83f8945e","executionInfo":{"status":"ok","timestamp":1555446752670,"user_tz":180,"elapsed":534,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","\n","# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n","print(num_train_steps)\n","print(num_warmup_steps)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["147\n","14\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1555446752923,"user_tz":180,"elapsed":399,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"q_WebpS1X97v","outputId":"7a7efd2e-69eb-4696-f018-d0e02592d8db","colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["model_fn = model_fn_builder(\n","    num_labels=len(label_list),\n","    learning_rate=LEARNING_RATE,\n","    num_train_steps=num_train_steps,\n","    num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","    model_fn=model_fn,\n","    config=run_config,\n","    params={\"batch_size\": BATCH_SIZE})"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7c20414710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:31.743960 140173275879296 estimator.py:201] Using config: {'_model_dir': 'model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7c20414710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4KCRIgjx1Hc-","colab_type":"text"},"source":["# 4.0 Training"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t6Nukby2EB6-"},"source":["Now we train our model! We can use a Colab notebook running on Google's GPUs, or just do it locally.\n","\n","Remember we built the estimator around the model_fn, now to call it we use the train_input_fn, we can also create other input functions and use them with the same estimator, depending on what data we want to pass (is it trian, test, val, adhoc predictions). All input functions look rather the same, as the diferentiation between training, evaluating, and predicting is determined by what method of the estimator we call, this sets a key (e.g. tf.estimator.ModeKeys.PREDICT) that our model_fn can deal with.\n","\n","WARNING: BERTs pretty fat (as previously discussed) if you get a OOM error, or Dst tensor is not initialized, or something weird it could well be your out of memory (VRAM), I tried running this on my work and home laptop (home laptop having 4GB VRAM) and still got OOM, if thats the case I reccomend running it in colab, im not sure how much you are alloted, I feel it varies depending on demand... but every time I have tried I was able to run this."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1555446953600,"user_tz":180,"elapsed":198335,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"nucD4gluYJmK","outputId":"10ffeac6-aab3-45ff-f919-946c7e98179a","colab":{"base_uri":"https://localhost:8080/","height":955}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:35.320011 140173275879296 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:38.320502 140173275879296 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-23-5751825f1cbf>:45: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["W0416 20:32:38.444983 140173275879296 deprecation.py:506] From <ipython-input-23-5751825f1cbf>:45: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["W0416 20:32:38.489684 140173275879296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0416 20:32:38.569790 140173275879296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0416 20:32:46.679609 140173275879296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:46.704024 140173275879296 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:46.709812 140173275879296 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:50.474211 140173275879296 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:55.909326 140173275879296 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:32:56.138591 140173275879296 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into model/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:33:04.371701 140173275879296 basic_session_run_hooks.py:594] Saving checkpoints for 0 into model/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 1.056304, step = 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:33:21.459272 140173275879296 basic_session_run_hooks.py:249] loss = 1.056304, step = 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.997685\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:35:01.690715 140173275879296 basic_session_run_hooks.py:680] global_step/sec: 0.997685\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.29937047, step = 100 (100.236 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:35:01.695717 140173275879296 basic_session_run_hooks.py:247] loss = 0.29937047, step = 100 (100.236 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 147 into model/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:35:42.705381 140173275879296 basic_session_run_hooks.py:594] Saving checkpoints for 147 into model/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.042803697.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:35:52.248720 140173275879296 estimator.py:359] Loss for final step: 0.042803697.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:03:17.767562\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CmbLTVniARy3"},"source":["Now let's use our test data to see how well our model did. Notice on the estimator call we call evaluate so our model function knows what to do (evaluate)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JIhejfpyJ8Bx","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1555446996249,"user_tz":180,"elapsed":25710,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"PPVEXhNjYXC-","outputId":"22ee3c67-f9d0-46db-d017-07491cfd3f9d","colab":{"base_uri":"https://localhost:8080/","height":564}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:10.000391 140173275879296 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:13.324335 140173275879296 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:21.721193 140173275879296 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-04-16T20:36:21Z\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:21.747836 140173275879296 evaluation.py:257] Starting evaluation at 2019-04-16T20:36:21Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:23.587167 140173275879296 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["W0416 20:36:23.598026 140173275879296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model/model.ckpt-147\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:23.602316 140173275879296 saver.py:1270] Restoring parameters from model/model.ckpt-147\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:26.098374 140173275879296 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:26.372336 140173275879296 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-04-16-20:36:32\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:32.299790 140173275879296 evaluation.py:277] Finished evaluation at 2019-04-16-20:36:32\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 147: eval_accuracy = 0.81333333, global_step = 147, loss = 0.4991908\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:32.302204 140173275879296 estimator.py:1979] Saving dict for global step 147: eval_accuracy = 0.81333333, global_step = 147, loss = 0.4991908\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 147: model/model.ckpt-147\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:34.906010 140173275879296 estimator.py:2039] Saving 'checkpoint_path' summary for global step 147: model/model.ckpt-147\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'eval_accuracy': 0.81333333, 'global_step': 147, 'loss': 0.4991908}"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"W5MibvY11HdW","colab_type":"text"},"source":["This called the evaluate method which can return a series of metrics automatically. However as mentioned there isnt the best choice of metrics available natively in Tensorflow so we will actually demo making predictions by making predictions on the whole test set and then evaluating our own metrics in Python.\n","\n","Also note: BERTs pretty good :o\n","\n","To make predictions we need a quick pipeline that does all of the data-prep stages we mentioned in this notebook, we can actually do that super cleanly when we dont spend ages pulling everything apart."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OsrbTD2EJTVl","colab":{}},"source":["def get_prediction(in_sentences):\n","    \"\"\"\n","    Helper function to run the pipeline for a list of examples to classify\n","    \"\"\"\n","    labels = [1,3,5]\n","    \n","    # As in 2.1. get input exaple objects in a list for every prediction example\n","    input_examples = [bert.run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 1) for x in in_sentences]\n","\n","    # As in 2.3 convert to the classes that BERT will ultimately understand (still in python format here)\n","    input_features = bert.run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","    \n","    # As in 3.2 we need an input function for the estimator, this just converts the data above to tensors to serve to the estimator\n","    predict_input_fn = bert.run_classifier.input_fn_builder(features=input_features, \n","                                                            seq_length=MAX_SEQ_LENGTH, \n","                                                            is_training=False, \n","                                                            drop_remainder=False)\n","    \n","    # Run the model this time on .predict which as defined by our model_fn will return probabilities and labels\n","    predictions = estimator.predict(predict_input_fn)\n","    \n","    # Returns a list of tuples (sentence, probs, label) for each example we passed\n","    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GdJJ1ekG1HdZ","colab_type":"code","outputId":"15c92a17-df5f-43c2-a54a-d2b15bad08c1","executionInfo":{"status":"ok","timestamp":1555447018850,"user_tz":180,"elapsed":12250,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"colab":{"base_uri":"https://localhost:8080/","height":1499}},"source":["predictions = get_prediction(df_tst[DATA_COLUMN])\n","pred_labels = [pred[2] for pred in predictions]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 450\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.795840 140173275879296 run_classifier.py:774] Writing example 0 of 450\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.802684 140173275879296 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.805588 140173275879296 run_classifier.py:462] guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] went here last night when on our last night staying at the hotel valley ho . we were able to walk there from the hotel and it was so cute . the wait staff were helpful and gr ##acious . once we saw how large the pizza ##s were , they acc ##omo ##date ##d us and split the topping ##s on each side of the pizza so we got what we wanted . i did think the olive ##s were really salty , but that ' s that type of olive . we had delicious wine and the br ##us ##chet ##ta sample ##r . enjoy people ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.808490 140173275879296 run_classifier.py:464] tokens: [CLS] went here last night when on our last night staying at the hotel valley ho . we were able to walk there from the hotel and it was so cute . the wait staff were helpful and gr ##acious . once we saw how large the pizza ##s were , they acc ##omo ##date ##d us and split the topping ##s on each side of the pizza so we got what we wanted . i did think the olive ##s were really salty , but that ' s that type of olive . we had delicious wine and the br ##us ##chet ##ta sample ##r . enjoy people ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2253 2182 2197 2305 2043 2006 2256 2197 2305 6595 2012 1996 3309 3028 7570 1012 2057 2020 2583 2000 3328 2045 2013 1996 3309 1998 2009 2001 2061 10140 1012 1996 3524 3095 2020 14044 1998 24665 20113 1012 2320 2057 2387 2129 2312 1996 10733 2015 2020 1010 2027 16222 19506 13701 2094 2149 1998 3975 1996 22286 2015 2006 2169 2217 1997 1996 10733 2061 2057 2288 2054 2057 2359 1012 1045 2106 2228 1996 9724 2015 2020 2428 23592 1010 2021 2008 1005 1055 2008 2828 1997 9724 1012 2057 2018 12090 4511 1998 1996 7987 2271 20318 2696 7099 2099 1012 5959 2111 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.811441 140173275879296 run_classifier.py:465] input_ids: 101 2253 2182 2197 2305 2043 2006 2256 2197 2305 6595 2012 1996 3309 3028 7570 1012 2057 2020 2583 2000 3328 2045 2013 1996 3309 1998 2009 2001 2061 10140 1012 1996 3524 3095 2020 14044 1998 24665 20113 1012 2320 2057 2387 2129 2312 1996 10733 2015 2020 1010 2027 16222 19506 13701 2094 2149 1998 3975 1996 22286 2015 2006 2169 2217 1997 1996 10733 2061 2057 2288 2054 2057 2359 1012 1045 2106 2228 1996 9724 2015 2020 2428 23592 1010 2021 2008 1005 1055 2008 2828 1997 9724 1012 2057 2018 12090 4511 1998 1996 7987 2271 20318 2696 7099 2099 1012 5959 2111 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.814660 140173275879296 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.817302 140173275879296 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.819468 140173275879296 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.840944 140173275879296 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.843656 140173275879296 run_classifier.py:462] guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i would give this place no stars but ye ##lp ! wants me to give at least one . i went here with my boyfriend last night for dinner and next to our awful bu ##cca di be ##pp ##o ( scott ##sdale ) experience , i have to say , this one is on my list of most terrible restaurant experiences . the service was so bad , we didn ' t even get to try the food ! true ye ##lp ##er that i am , i did my homework before suggesting that we come here for dinner . i saw all the reviews about the \" bad service \" , but i paid more attention to the \" cheap \" and \" delicious [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.846468 140173275879296 run_classifier.py:464] tokens: [CLS] i would give this place no stars but ye ##lp ! wants me to give at least one . i went here with my boyfriend last night for dinner and next to our awful bu ##cca di be ##pp ##o ( scott ##sdale ) experience , i have to say , this one is on my list of most terrible restaurant experiences . the service was so bad , we didn ' t even get to try the food ! true ye ##lp ##er that i am , i did my homework before suggesting that we come here for dinner . i saw all the reviews about the \" bad service \" , but i paid more attention to the \" cheap \" and \" delicious [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2052 2507 2023 2173 2053 3340 2021 6300 14277 999 4122 2033 2000 2507 2012 2560 2028 1012 1045 2253 2182 2007 2026 6898 2197 2305 2005 4596 1998 2279 2000 2256 9643 20934 16665 4487 2022 9397 2080 1006 3660 15145 1007 3325 1010 1045 2031 2000 2360 1010 2023 2028 2003 2006 2026 2862 1997 2087 6659 4825 6322 1012 1996 2326 2001 2061 2919 1010 2057 2134 1005 1056 2130 2131 2000 3046 1996 2833 999 2995 6300 14277 2121 2008 1045 2572 1010 1045 2106 2026 19453 2077 9104 2008 2057 2272 2182 2005 4596 1012 1045 2387 2035 1996 4391 2055 1996 1000 2919 2326 1000 1010 2021 1045 3825 2062 3086 2000 1996 1000 10036 1000 1998 1000 12090 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.849567 140173275879296 run_classifier.py:465] input_ids: 101 1045 2052 2507 2023 2173 2053 3340 2021 6300 14277 999 4122 2033 2000 2507 2012 2560 2028 1012 1045 2253 2182 2007 2026 6898 2197 2305 2005 4596 1998 2279 2000 2256 9643 20934 16665 4487 2022 9397 2080 1006 3660 15145 1007 3325 1010 1045 2031 2000 2360 1010 2023 2028 2003 2006 2026 2862 1997 2087 6659 4825 6322 1012 1996 2326 2001 2061 2919 1010 2057 2134 1005 1056 2130 2131 2000 3046 1996 2833 999 2995 6300 14277 2121 2008 1045 2572 1010 1045 2106 2026 19453 2077 9104 2008 2057 2272 2182 2005 4596 1012 1045 2387 2035 1996 4391 2055 1996 1000 2919 2326 1000 1010 2021 1045 3825 2062 3086 2000 1996 1000 10036 1000 1998 1000 12090 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.852427 140173275879296 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.855178 140173275879296 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.857872 140173275879296 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.870425 140173275879296 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.873100 140173275879296 run_classifier.py:462] guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] when fresh & easy started popping up everywhere , it was my new favorite go to place for quick and easy meals . they have a great selection of ready made en ##tree ##s and salad ##s that beat going through the drive through . the fact that there were always excellent coup ##ons to be received could have been a big factor in why i returned so often . key word , were , i think they have light ##ened up on their coup ##on circulation ! the problem started when i began to do my actual grocery shopping here . they have great prices on a limited , but satisfactory , amount of grocery items . its the produce and ve ##gg ##ies that [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.875387 140173275879296 run_classifier.py:464] tokens: [CLS] when fresh & easy started popping up everywhere , it was my new favorite go to place for quick and easy meals . they have a great selection of ready made en ##tree ##s and salad ##s that beat going through the drive through . the fact that there were always excellent coup ##ons to be received could have been a big factor in why i returned so often . key word , were , i think they have light ##ened up on their coup ##on circulation ! the problem started when i began to do my actual grocery shopping here . they have great prices on a limited , but satisfactory , amount of grocery items . its the produce and ve ##gg ##ies that [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2043 4840 1004 3733 2318 20095 2039 7249 1010 2009 2001 2026 2047 5440 2175 2000 2173 2005 4248 1998 3733 12278 1012 2027 2031 1037 2307 4989 1997 3201 2081 4372 13334 2015 1998 16521 2015 2008 3786 2183 2083 1996 3298 2083 1012 1996 2755 2008 2045 2020 2467 6581 8648 5644 2000 2022 2363 2071 2031 2042 1037 2502 5387 1999 2339 1045 2513 2061 2411 1012 3145 2773 1010 2020 1010 1045 2228 2027 2031 2422 6675 2039 2006 2037 8648 2239 9141 999 1996 3291 2318 2043 1045 2211 2000 2079 2026 5025 13025 6023 2182 1012 2027 2031 2307 7597 2006 1037 3132 1010 2021 23045 1010 3815 1997 13025 5167 1012 2049 1996 3965 1998 2310 13871 3111 2008 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.878395 140173275879296 run_classifier.py:465] input_ids: 101 2043 4840 1004 3733 2318 20095 2039 7249 1010 2009 2001 2026 2047 5440 2175 2000 2173 2005 4248 1998 3733 12278 1012 2027 2031 1037 2307 4989 1997 3201 2081 4372 13334 2015 1998 16521 2015 2008 3786 2183 2083 1996 3298 2083 1012 1996 2755 2008 2045 2020 2467 6581 8648 5644 2000 2022 2363 2071 2031 2042 1037 2502 5387 1999 2339 1045 2513 2061 2411 1012 3145 2773 1010 2020 1010 1045 2228 2027 2031 2422 6675 2039 2006 2037 8648 2239 9141 999 1996 3291 2318 2043 1045 2211 2000 2079 2026 5025 13025 6023 2182 1012 2027 2031 2307 7597 2006 1037 3132 1010 2021 23045 1010 3815 1997 13025 5167 1012 2049 1996 3965 1998 2310 13871 3111 2008 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.881414 140173275879296 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.883987 140173275879296 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.885993 140173275879296 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.889315 140173275879296 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.891434 140173275879296 run_classifier.py:462] guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] great experience at citizen ph . . . highly recommend the food and am ##bie ##nce trust me well worth the price ! ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.893851 140173275879296 run_classifier.py:464] tokens: [CLS] great experience at citizen ph . . . highly recommend the food and am ##bie ##nce trust me well worth the price ! ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2307 3325 2012 6926 6887 1012 1012 1012 3811 16755 1996 2833 1998 2572 11283 5897 3404 2033 2092 4276 1996 3976 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.896296 140173275879296 run_classifier.py:465] input_ids: 101 2307 3325 2012 6926 6887 1012 1012 1012 3811 16755 1996 2833 1998 2572 11283 5897 3404 2033 2092 4276 1996 3976 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.898450 140173275879296 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.901022 140173275879296 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.903170 140173275879296 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.920006 140173275879296 run_classifier.py:461] *** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.922582 140173275879296 run_classifier.py:462] guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] sometimes you just want to give an a for effort . . . or maybe more than three lame stars . as the two other previous reviewers have pointed out su ##cci ##nc ##tly , the man who i assume to be the owner was extremely nice . i made the 1 . 5 hour drive from tucson to watch avatar at arizona mills , and my friends and i decided to grab a bite before heading back . we randomly drove down baseline and pretty much stumbled upon this place . traffic going westbound towards the 10 was insane . i mean it was bumper to bumper past rural . anyway , i ask the guy who i assume is the owner why the traffic [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.925211 140173275879296 run_classifier.py:464] tokens: [CLS] sometimes you just want to give an a for effort . . . or maybe more than three lame stars . as the two other previous reviewers have pointed out su ##cci ##nc ##tly , the man who i assume to be the owner was extremely nice . i made the 1 . 5 hour drive from tucson to watch avatar at arizona mills , and my friends and i decided to grab a bite before heading back . we randomly drove down baseline and pretty much stumbled upon this place . traffic going westbound towards the 10 was insane . i mean it was bumper to bumper past rural . anyway , i ask the guy who i assume is the owner why the traffic [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2823 2017 2074 2215 2000 2507 2019 1037 2005 3947 1012 1012 1012 2030 2672 2062 2084 2093 20342 3340 1012 2004 1996 2048 2060 3025 15814 2031 4197 2041 10514 14693 12273 14626 1010 1996 2158 2040 1045 7868 2000 2022 1996 3954 2001 5186 3835 1012 1045 2081 1996 1015 1012 1019 3178 3298 2013 17478 2000 3422 22128 2012 5334 6341 1010 1998 2026 2814 1998 1045 2787 2000 6723 1037 6805 2077 5825 2067 1012 2057 18154 5225 2091 26163 1998 3492 2172 9845 2588 2023 2173 1012 4026 2183 24717 2875 1996 2184 2001 9577 1012 1045 2812 2009 2001 21519 2000 21519 2627 3541 1012 4312 1010 1045 3198 1996 3124 2040 1045 7868 2003 1996 3954 2339 1996 4026 102\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.927922 140173275879296 run_classifier.py:465] input_ids: 101 2823 2017 2074 2215 2000 2507 2019 1037 2005 3947 1012 1012 1012 2030 2672 2062 2084 2093 20342 3340 1012 2004 1996 2048 2060 3025 15814 2031 4197 2041 10514 14693 12273 14626 1010 1996 2158 2040 1045 7868 2000 2022 1996 3954 2001 5186 3835 1012 1045 2081 1996 1015 1012 1019 3178 3298 2013 17478 2000 3422 22128 2012 5334 6341 1010 1998 2026 2814 1998 1045 2787 2000 6723 1037 6805 2077 5825 2067 1012 2057 18154 5225 2091 26163 1998 3492 2172 9845 2588 2023 2173 1012 4026 2183 24717 2875 1996 2184 2001 9577 1012 1045 2812 2009 2001 21519 2000 21519 2627 3541 1012 4312 1010 1045 3198 1996 3124 2040 1045 7868 2003 1996 3954 2339 1996 4026 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.930541 140173275879296 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.932694 140173275879296 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:45.935087 140173275879296 run_classifier.py:468] label: 1 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:47.454122 140173275879296 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:51.081074 140173275879296 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:51.256431 140173275879296 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:51.763249 140173275879296 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from model/model.ckpt-147\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:51.774970 140173275879296 saver.py:1270] Restoring parameters from model/model.ckpt-147\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:52.588487 140173275879296 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0416 20:36:52.678157 140173275879296 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5Y6DPEZo1Hdb","colab_type":"text"},"source":["Now the below is just an adhoc example of what we can do to evaluate this"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1555447025172,"user_tz":180,"elapsed":1317,"user":{"displayName":"Peter Usherwood","photoUrl":"https://lh4.googleusercontent.com/-UfWGFihAdag/AAAAAAAAAAI/AAAAAAAAAI4/dINHEXgZkiQ/s64/photo.jpg","userId":"11459160037307263961"}},"id":"BXw2BE5IzzvH","outputId":"442b70b7-e0a0-4114-bc5a-d468260e1c18","colab":{"base_uri":"https://localhost:8080/","height":396}},"source":["import itertools\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","\n","    :param cm: Confusion matrix\n","    :param classes: Python list of class names\n","    :param normalize: Boolean to normalize the matrix\n","    :param title: Title of the graph\n","    :param cmap: Colour map\n","    \"\"\"\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, \"{:.2f}\".format(cm[i, j]),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","\n","cm_test = confusion_matrix(y_true=df_tst[\"label\"], y_pred=pred_labels)\n","plot_confusion_matrix(cm_test,\n","                      ['neg', 'neu', 'pos'],\n","                      normalize=False,\n","                      title='Validation')\n","print(\"f1 score\", f1_score(df_tst[\"label\"], pred_labels, average=\"micro\"))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Confusion matrix, without normalization\n","[[128  19   3]\n"," [ 19 116  15]\n"," [  2  26 122]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAU4AAAEmCAYAAAAN9HleAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["f1 score 0.8133333333333334\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ueKsULteiz1B"},"source":["Now we can predict completely made up adhoc examples:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-thbodgih_VJ","colab":{}},"source":["pred_sentences = [\n","  \"That movie was absolutely awful\",\n","  \"The acting was a bit lacking\",\n","  \"The film was creative and surprising\",\n","  \"Absolutely fantastic!\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QrZmvZySKQTm","colab":{}},"source":["predictions = getPrediction(pred_sentences)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MXkRiEBUqN3n"},"source":["Voila! We have a sentiment classifier!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ERkTE8-7oQLZ","colab":{}},"source":["predictions[0][2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bjPMaH1FvomF","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}